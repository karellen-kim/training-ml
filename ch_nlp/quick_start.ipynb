{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e709ba6c-5ed3-4b61-b3b5-b72331410818",
   "metadata": {},
   "source": [
    "## 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5de976-af8c-4325-97b9-1d5a3c3c368d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-04-21T17:30:35.468339Z",
     "end_time": "2023-04-21T17:30:39.971746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "682a54e3a0204a3ebfcfd7afcba7e7a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunmi.kim/Library/Caches/pypoetry/virtualenvs/pythonproject-UeCig8hf-py3.9/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\") # 기본 모델 T5-base 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'translation_text': \"Le soleil s'élève dans les collines.\"}]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"The sun is rising amid the hills.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:53:13.381492Z",
     "end_time": "2023-04-09T19:53:15.750145Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "8e8895a5-223f-4a9a-9267-edd65287e855",
   "metadata": {},
   "source": [
    "## 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e8ae20-8b07-40ee-b66c-daace786717a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-04-09T19:53:15.752146Z",
     "end_time": "2023-04-09T19:53:48.911363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████| 629/629 [00:00<00:00, 376kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:30<00:00, 8.93MB/s] \n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 47.5kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 7.69MB/s]\n"
     ]
    }
   ],
   "source": [
    "textClassifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'NEGATIVE', 'score': 0.9846920371055603}]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textClassifier(\"Call me Ishmael. \\\n",
    "Some years ago—never mind how long precisely—having little \\\n",
    "or no money in my purse, and nothing particular to interest me on shore, \\\n",
    "I thought I would sail about a little and see the watery part of the world. \\\n",
    "It is a way I have of driving off the spleen, and regulating the circulation.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:53:48.912205Z",
     "end_time": "2023-04-09T19:53:49.584379Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'NEGATIVE', 'score': 0.8103969097137451},\n {'label': 'POSITIVE', 'score': 0.9776003956794739}]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textClassifier([\"To be, or not to be\",\"With mirth and laughter let old wrinkles come\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:53:49.588741Z",
     "end_time": "2023-04-09T19:53:49.922638Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 텍스트 추론 (Natural Language Inference : NLI)\n",
    "* 문장이 참(entailment), 거짓(contradiction), 중립(neutral)인지 가려낸다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 688/688 [00:00<00:00, 864kB/s]\n",
      "Downloading: 100%|██████████| 1.43G/1.43G [02:50<00:00, 8.37MB/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 6.61MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 2.66MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "nliClassifier = pipeline(\"text-classification\", model = \"roberta-large-mnli\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:53:49.922940Z",
     "end_time": "2023-04-09T19:56:46.245904Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'ENTAILMENT', 'score': 0.7261579036712646}]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nliClassifier(\"Staying clean is a good thing. Hygiene is a lovely thing.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:56:46.246513Z",
     "end_time": "2023-04-09T19:56:47.115544Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'CONTRADICTION', 'score': 0.965621829032898}]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nliClassifier(\"Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. I have a waterphobia.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:56:47.115447Z",
     "end_time": "2023-04-09T19:56:48.515459Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'NEUTRAL', 'score': 0.5616682767868042}]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nliClassifier(\"Cats were represented in social and religious practices of ancient Egypt for more than 3,000 years. Power systems is a major subfield of Electrical Engineering.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:56:48.521784Z",
     "end_time": "2023-04-09T19:56:49.645332Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QNLI\n",
    "* 문장이 가설과 관련된 정보를 포함하고 있는지 알 수 있다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 771/771 [00:00<00:00, 210kB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [00:53<00:00, 8.14MB/s] \n",
      "Downloading: 100%|██████████| 268/268 [00:00<00:00, 111kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 11.1MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 27.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "qnliClassifier = pipeline(\"text-classification\", model = \"cross-encoder/qnli-electra-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:56:49.648420Z",
     "end_time": "2023-04-09T19:57:46.930377Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'LABEL_0', 'score': 0.002537613268941641}]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnliClassifier(\"What's the major subfield of Electrical Engineering?, Staying clean is a good thing.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:57:46.931136Z",
     "end_time": "2023-04-09T19:57:47.269576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'LABEL_0', 'score': 0.7401793599128723}]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnliClassifier(\"What's the major subfield of Electrical Engineering?, Power and Electronics engineerings are usually preferred by new undergrad students.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:57:47.277535Z",
     "end_time": "2023-04-09T19:57:47.669424Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question Answering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████| 473/473 [00:00<00:00, 90.4kB/s]\n",
      "Downloading: 100%|██████████| 261M/261M [00:34<00:00, 7.50MB/s]   \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 5.60kB/s]\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 1.18MB/s]\n",
      "Downloading: 100%|██████████| 436k/436k [00:00<00:00, 7.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "qaModel = pipeline(\"question-answering\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:57:47.669349Z",
     "end_time": "2023-04-09T19:58:27.069964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.9327439069747925, 'start': 111, 'end': 115, 'answer': 'grey'}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage = \"THE TIME TRAVELLER (for so it will be convenient to speak of him) \\\n",
    "was expounding a recondite matter to us. \\\n",
    "His grey eyes shone and twinkled, and his usually pale face was flushed \\\n",
    "and animated. The fire burned brightly, and the soft radiance of the \\\n",
    "incandescent lights in the lilies of silver caught the bubbles \\\n",
    "that flashed and passed in our glasses. \\\n",
    "Our chairs, being his patents, embraced and caressed us rather than submitted \\\n",
    "to be sat upon, and there was that luxurious after-dinner atmosphere \\\n",
    "when thought runs gracefully free of the trammels of precision. \\\n",
    "And he put it to us in this way—marking the points with a lean forefinger—as \\\n",
    "we sat and lazily admired his earnestness over this new paradox \\\n",
    "(as we thought it) and his fecundity.\"\n",
    "\n",
    "qaModel(\"What was colour of time traveller eyes?\", passage)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:58:27.074645Z",
     "end_time": "2023-04-09T19:58:27.561658Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.0685771033167839, 'start': 351, 'end': 361, 'answer': 'Our chairs'}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qaModel(\"Where were we sitting?\", passage)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:58:27.560277Z",
     "end_time": "2023-04-09T19:58:28.143413Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.03444063290953636,\n 'start': 107,\n 'end': 139,\n 'answer': 'His grey eyes shone and twinkled'}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qaModel(\"What was our reaction to time traveller?\", passage)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:58:28.151508Z",
     "end_time": "2023-04-09T19:58:28.698788Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 문법 수정하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 490/490 [00:00<00:00, 129kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:34<00:00, 7.81MB/s]   \n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 9.83kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 5.53MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 24.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "grammarClassifier = pipeline(\"text-classification\", model = \"textattack/distilbert-base-uncased-CoLA\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:58:28.703046Z",
     "end_time": "2023-04-09T19:59:07.680197Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'LABEL_0', 'score': 0.6497102975845337}]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammarClassifier(\"he go to school everyday out missing the bus\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:59:07.667489Z",
     "end_time": "2023-04-09T19:59:08.016984Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'LABEL_1', 'score': 0.9765012860298157}]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammarClassifier(\"This chapter will provide an overview of performing common NLP tasks.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T19:59:08.015997Z",
     "end_time": "2023-04-09T19:59:08.519498Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 토큰 분류\n",
    "* 2가지 step이 있다\n",
    "  * Named Entity Recognition (NER)\n",
    "  * Part-of-Speech (PoS) tagging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5e1067d-69bf-4d6a-91c3-d78f47156d8e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-09T19:59:08.517995Z",
     "end_time": "2023-04-09T20:02:04.098607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 206kB/s]\n",
      "Downloading: 100%|██████████| 1.33G/1.33G [02:50<00:00, 7.83MB/s]\n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 12.0kB/s]\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 1.29MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenClassifier = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entity': 'I-PER',\n  'score': 0.9345262,\n  'index': 1,\n  'word': 'Ptolemy',\n  'start': 0,\n  'end': 7},\n {'entity': 'I-MISC',\n  'score': 0.9682059,\n  'index': 5,\n  'word': 'G',\n  'start': 24,\n  'end': 25},\n {'entity': 'I-MISC',\n  'score': 0.8515835,\n  'index': 6,\n  'word': '##eo',\n  'start': 25,\n  'end': 27},\n {'entity': 'I-MISC',\n  'score': 0.86348933,\n  'index': 7,\n  'word': '##graph',\n  'start': 27,\n  'end': 32},\n {'entity': 'I-MISC',\n  'score': 0.9286009,\n  'index': 8,\n  'word': '##ia',\n  'start': 32,\n  'end': 34},\n {'entity': 'I-LOC',\n  'score': 0.991808,\n  'index': 12,\n  'word': 'Lab',\n  'start': 49,\n  'end': 52},\n {'entity': 'I-LOC',\n  'score': 0.9888537,\n  'index': 13,\n  'word': '##ok',\n  'start': 52,\n  'end': 54},\n {'entity': 'I-LOC',\n  'score': 0.9976913,\n  'index': 14,\n  'word': '##la',\n  'start': 54,\n  'end': 56},\n {'entity': 'I-LOC',\n  'score': 0.99820256,\n  'index': 23,\n  'word': 'Lahore',\n  'start': 101,\n  'end': 107}]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenClassifier(\"Ptolemy mentions in his Geographia a city called Labokla which may have been in reference to ancient Lahore.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:02:04.094879Z",
     "end_time": "2023-04-09T20:02:05.537916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.06k/1.06k [00:00<00:00, 345kB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [01:00<00:00, 7.21MB/s] \n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 24.7kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 7.22MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 16.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "posTagger = pipeline(\"token-classification\", model = \"vblagoje/bert-english-uncased-finetuned-pos\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:02:05.537203Z",
     "end_time": "2023-04-09T20:03:10.675783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entity': 'PROPN',\n  'score': 0.9985763,\n  'index': 1,\n  'word': 'ptolemy',\n  'start': 0,\n  'end': 7},\n {'entity': 'VERB',\n  'score': 0.99952936,\n  'index': 2,\n  'word': 'mentions',\n  'start': 8,\n  'end': 16},\n {'entity': 'ADP',\n  'score': 0.9994611,\n  'index': 3,\n  'word': 'in',\n  'start': 17,\n  'end': 19},\n {'entity': 'PRON',\n  'score': 0.998919,\n  'index': 4,\n  'word': 'his',\n  'start': 20,\n  'end': 23},\n {'entity': 'PROPN',\n  'score': 0.59625757,\n  'index': 5,\n  'word': 'geo',\n  'start': 24,\n  'end': 27},\n {'entity': 'PROPN',\n  'score': 0.77627975,\n  'index': 6,\n  'word': '##graph',\n  'start': 27,\n  'end': 32},\n {'entity': 'NOUN',\n  'score': 0.6261003,\n  'index': 7,\n  'word': '##ia',\n  'start': 32,\n  'end': 34},\n {'entity': 'DET',\n  'score': 0.9993963,\n  'index': 8,\n  'word': 'a',\n  'start': 35,\n  'end': 36},\n {'entity': 'NOUN',\n  'score': 0.9986626,\n  'index': 9,\n  'word': 'city',\n  'start': 37,\n  'end': 41},\n {'entity': 'VERB',\n  'score': 0.9963213,\n  'index': 10,\n  'word': 'called',\n  'start': 42,\n  'end': 48},\n {'entity': 'PROPN',\n  'score': 0.997322,\n  'index': 11,\n  'word': 'lab',\n  'start': 49,\n  'end': 52},\n {'entity': 'PROPN',\n  'score': 0.9933227,\n  'index': 12,\n  'word': '##ok',\n  'start': 52,\n  'end': 54},\n {'entity': 'PROPN',\n  'score': 0.9967691,\n  'index': 13,\n  'word': '##la',\n  'start': 54,\n  'end': 56},\n {'entity': 'PRON',\n  'score': 0.9993855,\n  'index': 14,\n  'word': 'which',\n  'start': 57,\n  'end': 62},\n {'entity': 'AUX',\n  'score': 0.9985049,\n  'index': 15,\n  'word': 'may',\n  'start': 63,\n  'end': 66},\n {'entity': 'AUX',\n  'score': 0.9961761,\n  'index': 16,\n  'word': 'have',\n  'start': 67,\n  'end': 71},\n {'entity': 'AUX',\n  'score': 0.99127114,\n  'index': 17,\n  'word': 'been',\n  'start': 72,\n  'end': 76},\n {'entity': 'ADP',\n  'score': 0.99933344,\n  'index': 18,\n  'word': 'in',\n  'start': 77,\n  'end': 79},\n {'entity': 'NOUN',\n  'score': 0.9973694,\n  'index': 19,\n  'word': 'reference',\n  'start': 80,\n  'end': 89},\n {'entity': 'ADP',\n  'score': 0.9994091,\n  'index': 20,\n  'word': 'to',\n  'start': 90,\n  'end': 92},\n {'entity': 'ADJ',\n  'score': 0.99520975,\n  'index': 21,\n  'word': 'ancient',\n  'start': 93,\n  'end': 100},\n {'entity': 'PROPN',\n  'score': 0.9989177,\n  'index': 22,\n  'word': 'lahore',\n  'start': 101,\n  'end': 107},\n {'entity': 'PUNCT',\n  'score': 0.9996501,\n  'index': 23,\n  'word': '.',\n  'start': 107,\n  'end': 108}]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posTagger(\"Ptolemy mentions in his Geographia a city called Labokla which may have been in reference to ancient Lahore.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:03:10.670018Z",
     "end_time": "2023-04-09T20:03:11.197828Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 텍스트 요약"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████| 1.80k/1.80k [00:00<00:00, 355kB/s]\n",
      "Downloading: 100%|██████████| 1.22G/1.22G [02:28<00:00, 8.22MB/s]\n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 13.3kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 5.99MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 9.68MB/s]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\") # default로 distilbart-cnn-12-6 모델 사용"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:03:11.194624Z",
     "end_time": "2023-04-09T20:05:47.545341Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'summary_text': ' There are really four dimensions, three which we call the three planes of Space, and a fourth, Time . There is a tendency to'}]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\"“Filby became pensive. “Clearly,” the Time Traveller proceeded, “any real body must have extension in four directions: it must have Length, Breadth, Thickness, and—Duration. But through a natural infirmity of the flesh, which I will explain to you in a moment, we incline to overlook this fact. There are really four dimensions, three which we call the three planes of Space, and a fourth, Time. There is, however, a tendency to draw an unreal distinction between the former three dimensions and the latter, because it happens that our consciousness moves intermittently in one direction along the latter from the beginning to the end of our lives.”\", min_length=10, max_length=30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:05:47.546225Z",
     "end_time": "2023-04-09T20:05:54.197404Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 다른 모델로 요약하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.58k/1.58k [00:00<00:00, 565kB/s]\n",
      "Downloading: 100%|██████████| 1.63G/1.63G [03:14<00:00, 8.38MB/s]  \n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 6.93MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 823kB/s] \n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.11MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'summary_text': '\"There are really four dimensions, three which we call the three planes of Space, and a fourth, Time\" \"Our consciousness moves'}]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bartLargeSummarizer = pipeline(\"summarization\", \"facebook/bart-large-cnn\")\n",
    "bartLargeSummarizer(\"“Filby became pensive. “Clearly,” the Time Traveller proceeded, “any real body must have extension in four directions: it must have Length, Breadth, Thickness, and—Duration. But through a natural infirmity of the flesh, which I will explain to you in a moment, we incline to overlook this fact. There are really four dimensions, three which we call the three planes of Space, and a fourth, Time. There is, however, a tendency to draw an unreal distinction between the former three dimensions and the latter, because it happens that our consciousness moves intermittently in one direction along the latter from the beginning to the end of our lives.”\", min_length=10, max_length=30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:05:54.202180Z",
     "end_time": "2023-04-09T20:14:22.652872Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 73.3kB/s]\n",
      "Downloading: 100%|██████████| 548M/548M [01:06<00:00, 8.23MB/s] \n",
      "Downloading: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.83MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 2.49MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.03MB/s]\n"
     ]
    }
   ],
   "source": [
    "gpt2Generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:14:22.657493Z",
     "end_time": "2023-04-09T20:15:35.126020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': \"Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. \\xa0For this project, we'll use an original neural network and an artificial neural network. \\xa0So imagine you're just starting out. \\xa0You take a series of small blocks of text and turn them into a series of larger blocks. \\xa0Now, imagine you're just learning something and you're just getting the blocks out of your head. \\xa0How do you do that? \\xa0Well, you'll use your neural networks to train this new neural network to learn words, words come to you in different ways. \\xa0We'll train this machine to pick up a letter and use phrases on a given paragraph. \\xa0The basic idea for this approach is to keep your neural network on task the moment the letter is written. This is called learning. \\xa0When a machine's neural network learns about a word\"},\n {'generated_text': \"Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. \\xa0This approach uses high temporal contrast, linear coloration, or both, to model the human reading task and makes it even more powerful than previous models. \\xa0Thus it is considered a viable first step in developing the first predictive language for reading to text. \\xa0A very novel and unique model is named the RNP (Random Access Research Materials). \\xa0It focuses on text for a user's ability to recall and learn information about real and artificial life. \\xa0It uses deep learning to perform the task in two ways: for understanding the content and a combination of different input-output systems of human-like speech. The RNP allows the users to process information that can be quickly processed and processed by the computer. \\xa0In addition, there is a special mode that works to provide more information: by selecting the specific\"},\n {'generated_text': 'Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. \\xa0GPT-3 includes a key set of novel features that have recently been discovered: the ability to create words without using a deep learning framework; the ability to use language--specific training algorithms that can perform speech prediction efficiently; and a number of new features that combine computational capabilities of computer-generated speech synthesizers, neural networks, and deep learning algorithms to form a language that can be spoken and written with no training, as opposed to simply \"reperceiving\" a speech message from a speaker who doesn\\'t understand it. The GPT-3 model is used to develop various language features that can become more complex over time (e.g., a complex vocabulary might be modeled as a simple mathematical statement such as\\n\\n/\\\\mathbb{\\\\infty} {/^\\\\infty } \\\\textrm{'}]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2Generator(\"Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. \", max_length = 200, num_return_sequences=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:15:35.126710Z",
     "end_time": "2023-04-09T20:15:52.267020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "text2textGenerator = pipeline(\"text2text-generation\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:15:52.272529Z",
     "end_time": "2023-04-09T20:15:53.849262Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'generated_text': 'Physicist'}]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2textGenerator(\"question: Who was Feynman ? context: Richard Feynman was a Physicist. Being one of the most famous scientist ever, he is still remembered in the scientific society.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:15:53.850144Z",
     "end_time": "2023-04-09T20:15:54.720713Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fill Mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████| 480/480 [00:00<00:00, 144kB/s]\n",
      "Downloading: 100%|██████████| 331M/331M [00:42<00:00, 7.81MB/s] \n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 9.58MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 8.80MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.62MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'score': 0.2632535994052887,\n  'token': 4811,\n  'token_str': ' foundation',\n  'sequence': 'DNA is the foundation of life.'},\n {'score': 0.11198728531599045,\n  'token': 1453,\n  'token_str': ' basis',\n  'sequence': 'DNA is the basis of life.'},\n {'score': 0.06848315894603729,\n  'token': 22157,\n  'token_str': ' spice',\n  'sequence': 'DNA is the spice of life.'},\n {'score': 0.03729118034243584,\n  'token': 14981,\n  'token_str': ' essence',\n  'sequence': 'DNA is the essence of life.'},\n {'score': 0.03439263254404068,\n  'token': 9813,\n  'token_str': ' origin',\n  'sequence': 'DNA is the origin of life.'}]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskFiller = pipeline(\"fill-mask\")\n",
    "maskFiller(\"DNA is the <mask> of life.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T20:15:54.725326Z",
     "end_time": "2023-04-09T20:16:43.657790Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
